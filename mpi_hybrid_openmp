#include <mpi.h>
#include <iostream>
#include <cstdlib>
#include <omp.h>
#include <chrono>

#define N 1000

int main(int argc, char** argv) {
    int rank, size;
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    int A[N][N], B[N][N], C[N][N];

    // Initialize matrices A and B
    for (int i = 0; i < N; i++) {
        for (int j = 0; j < N; j++) {
            A[i][j] = rand() % 10;
            B[i][j] = rand() % 10;
        }
    }

    // Scatter matrix A to all processes
    int blocksize = N / size;
    int buffer[blocksize][N];
    MPI_Scatter(A, blocksize * N, MPI_INT, buffer, blocksize * N, MPI_INT, 0, MPI_COMM_WORLD);

    // Broadcast matrix B to all processes
    MPI_Bcast(B, N*N, MPI_INT, 0, MPI_COMM_WORLD);

    // Perform local matrix multiplication using OpenMP
    auto start = std::chrono::high_resolution_clock::now();
    #pragma omp parallel for
    for (int i = 0; i < blocksize; i++) {
        for (int j = 0; j < N; j++) {
            C[i + rank*blocksize][j] = 0;
            for (int k = 0; k < N; k++) {
                C[i + rank*blocksize][j] += buffer[i][k] * B[k][j];
            }
        }
    }
    auto stop = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(stop - start);

    // Gather results from all processes
    MPI_Gather(C + rank*blocksize, blocksize * N, MPI_INT, C, blocksize * N, MPI_INT, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        // Print resulting matrix C
        std::cout << "Resulting matrix C:" << std::endl;
        for (int i = 0; i < N; i++) {
            for (int j = 0; j < N; j++) {
                std::cout << C[i][j] << " ";
            }
            std::cout << std::endl;
        }
        std::cout << "Execution time: " << duration.count() << " milliseconds" << std::endl;
    }

    MPI_Finalize();
    return 0;
}
